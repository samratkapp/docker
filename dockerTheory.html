<pre>

	Docker gives you a standard way of packaging your application with all its dependencies in a container

	Containers allow a developer to package up an application with all of the parts it needs, such as libraries and other dependencies, and ship it all out as one package.

	It provides a vertualization for your software. Like java application can run on any platform having JVM. A dockerised software can run anywhere having docker installed.
	They have insperation from LXC (Linux Containers) which is an operating-system-level virtualization method for running multiple isolated Linux systems (containers) on a control host using a single Linux kernel.
	
	Docker debuted to the public in Santa Clara at PyCon in 2013.[12] It was released as open-source in March 2013. At the time, it used LXC as its default execution environment. One year later, with the release of version 0.9, Docker replaced LXC with its own component, which was written in the Go programming language.

	Docker Inc. was founded by Solomon Hykes and Sebastien Pahl during the Y Combinator Summer 2010 startup incubator group and launched in 2011.[10] Hykes started the Docker project in France as an internal project within dotCloud, a platform-as-a-service company.[11]


# LXC (Linux Containers) is an operating-system-level virtualization method for running multiple isolated Linux systems (containers) on a control host using a single Linux kernel.
LXC provides operating system-level virtualization through a virtual environment that has its own process and network space, instead of creating a full-fledged virtual machine.
	The Linux kernel provides the cgroups functionality that allows limitation and prioritization of resources (CPU, memory, block I/O, network, etc.) without the need for starting any virtual machines, and also namespace isolation functionality that allows complete isolation of an application's view of the operating environment, including process trees, networking, user IDs and mounted file systems.[3]
		
	LXC combines the kernel's cgroups and support for isolated namespaces to provide an isolated environment for applications. Early versions of Docker used LXC as the container execution driver, though LXC was made optional in v0.9 and support was dropped in Docker v1.10. [4]
		
	Concept :
 
	Debug your app, not your environment
	Securely build and share any application, anywhere

	Containerize and share any application
	Across any combination of clouds, languages, and frameworks
	
	It’s not possible to explain what Docker is without explaining what containers are, so let’s look at a quick explanation of containers and how they work.

A container is a special type of process that is isolated from other processes. Containers are assigned resources that no other process can access, and they cannot access any resources not explicitly assigned to them.

# Why would I want to use Docker?
Imagine you are working on an analysis in R and you send your code to a friend. Your friend runs exactly this code on exactly the same data set but gets a slightly different result. This can have various reasons such as a different operating system, a different version of an R package, et cetera. Docker is trying to solve problems like that.

# A Docker container can be seen as a computer inside your computer ::. The cool thing about this virtual computer is that you can send it to your friends; And when they start this computer and run your code they will get exactly the same results as you did.
	Docker :
		What ?
		Need ?
		Why ? 
		Problems before docker :
		1. Application run / working in one environment but not in other.
		   e.g There can be a software wich is updated on Prod but remain same on dev.
		2. Microservice and vertual machine

		Feature ?
		benefit ?
		Benefits of DockerPermalink
		Reproducibility: Similar to a Java application, which will run exactly the same on any device capable of running a Java Virtual Machine, a Docker container is guaranteed to be identical on any system that can run Docker. The exact specifications of a container are stored in a Dockerfile. By distributing this file among team members, an organization can guarantee that all images built from the same Dockerfile will function identically. In addition, having an environment that is constant and well-documented makes it easier to keep track of your application and identify problems.

		Isolation: Dependencies or settings within a container will not affect any installations or configurations on your computer, or on any other containers that may be running. By using separate containers for each component of an application (for example a web server, front end, and database for hosting a web site), you can avoid conflicting dependencies. You can also have multiple projects on a single server without worrying about creating conflicts on your system.

		Security: With important caveats (discussed below), separating the different components of a large application into different containers can have security benefits: if one container is compromised the others remain unaffected.

		Docker Hub: For common or simple use cases, such as a LAMP stack, the ability to save images and push them to Docker Hub means that there are already many well-maintained images available. Being able to quickly pull a premade image or build from an officially-maintained Dockerfile can make this kind of setup process extremely fast and simple.

		Environment Management: Docker makes it easy to maintain different versions of, for example, a website using nginx. You can have a separate container for testing, development, and production on the same Linode and easily deploy to each one.

		Continuous Integration: Docker works well as part of continuous integration pipelines with tools like Travis, Jenkins, and Wercker. Every time your source code is updated, these tools can save the new version as a Docker image, tag it with a version number and push to Docker Hub, then deploy it to production.
	
		Docker essentially takes your operating system and split into many self-contained areas where applications can run in. Some say that it’s more like a virtual machine but in my opinion, even though there are some similarities, there are so many things that aren’t the same.
What are Virtual Machines?
Virtual machines (VMs) are an abstraction of physical hardware turning one server into many servers. It is more like taking a single operating system and divide it into small operating systems, each one thinking that they are running on their own system.
The problem with traditional virtual machines is that they are heavyweight, it takes a lot of resources and you can’t run too many on a single system because it overloads everything. This is where the containers come into play.
What is a Container?
A container is a standardized unit of software. It helps to distribute the software regardless of the setup. It packages up code and all its dependencies so the application runs quickly and reliably from one computing environment to another. It is used to run and test the software the same way everywhere you run it.

The Difference Between a Container and a Virtual Machine (Source: https://www.docker.com/blog/containers-replacing-virtual-machines/)
VMs are built on top of the HostOS, accordingly adding a layer, which is completely eluded in containers.
As a programmer, why you should be concerned about it?
Docker is a platform for developers and sysadmins to build, share and run applications with containers. It is ideal to use Docker cause of many reasons.
Flexibility — Even the most complex applications can be containerized.
Lightweight — Containers share the host kernel, making them much more efficient in terms of system resources than virtual machines.
Portable — You can build locally, deploy to the cloud, and run anywhere.
Scalable — You can increase and automatically distribute container replicas across a datacenter.
Secure — Applications are safer in containers and Docker provides the strongest default isolation capabilities.
Docker Hub — Docker hub acts as the app store for all Docker images. It contains thousands of images created for the community and readily available to use.
Docker Terminology
Docker image — The binaries, libraries and source code that all makes up an application
Docker container — A running instance of the image
Docker network — Each container is connected to a private virtual network called “bridge”
Dockerfile — Describes the software that makes up an image. Dockerfiles contain a set of instructions that specify what environment to use and which commands to run.
Docker-compose — Docker compose uses YAML files to configure and run containers. This means that we can ship our application Dockerfile to build the environment and use a docker-compose.yml to run the containers.
	Ease operations of docker by dockerfile.

	# DOCKER HUB 
	Docker Hub is a registry service on the cloud .
	build upload/download images.

	The world’s leading service for finding and sharing container images with your team and the Docker community.
    For developers and those experimenting with Docker, Docker Hub is your starting point into Docker containers. Create an account and start exploring the millions of images that are available from the community and verified publishers.

#	What is Docker Architecture?

#	Why is Docker so popular and why the rise of containers?
1. Ease of use
A large part of Docker’s popularity is how easy it is to use. Docker can be learned quickly, mainly due to the many resources available to learn how to create and manage containers. Docker is open-source, so all you need to get started is a computer with an operating system that supports Virtualbox, Docker for Mac/Windows, or supports containers natively, such as Linux.

2. Faster scaling of systems
Containers allow much more work to be done by far less computing hardware. In the early days of the Internet, the only way to scale a website was to buy or lease more servers. The cost of popularity was bound, linearly, to the cost of scaling up. Popular sites became victims of their own success, shelling out tens of thousands of dollars for new hardware. Containers allow data center operators to cram far more workloads into less hardware. Shared hardware means lower costs. Operators can bank those profits or pass the savings along to their customers.

3. Better software delivery
Software delivery using containers can also be more efficient. Containers are portable. They are also entirely self-contained. Containers include an isolated disk volume. That volume goes with the container as it is developed and deployed to various environments. The software dependencies (libraries, runtimes, etc.) ship with the container. If a container works on your machine, it will run the same way in a Development, Staging, and Production environment. Containers can eliminate the configuration variance problems common when deploying binaries or raw code.

4. Flexibility
Operating containerized applications is more flexible and resilient than that of non-containerized applications. Container orchestrators handle the running and monitoring of hundreds or thousands of containers.

Container orchestrators are very powerful tools for managing large deployments and complex systems. Perhaps the only thing more popular than Docker right now is Kubernetes, currently the most popular container orchestrator.

5. Software-defined networking
Docker supports software-defined networking. The Docker CLI and Engine allow operators to define isolated networks for containers, without having to touch a single router. Developers and operators can design systems with complex network topologies and define the networks in configuration files. This is a security benefit, as well. An application’s containers can run in an isolated virtual network, with tightly-controlled ingress and egress paths.

6. The rise of microservices architecture
The rise of microservices has also contributed to the popularity of Docker. Microservices are simple functions, usually accessed via HTTP/HTTPS, that do one thing — and do it well.

Software systems typically start as “monoliths,” in which a single binary supports many different system functions. As they grow, monoliths can become difficult to maintain and deploy. Microservices break a system down into simpler functions that can be deployed independently. Containers are terrific hosts for microservices. They are self-contained, easily deployed, and efficient.



# IMAGES
A Docker Image is nothing but a blueprint to deploy multiple containers of the same configurations
	$ docker images  (list of Docker images on the system)
	$ docker pull <image-name> (e.g jenkins) 

	$ docker build --no-cache -t u12_core -f u12_core .	

# CONTAINER
According to Docker, a container is a lightweight, stand-alone, executable package of a piece of software that includes everything needed to run it.
Containers are platform-independent and hence Docker can run across both Windows and Linux-based platforms.
The main purpose of Docker is that it lets you run microservice applications in a distributed architecture.
	$ docker run -p 8080:8080 -p 50000:50000 jenkins 
	$ docker top ContainerID
	$ docker stop ContainerID
	$ docker start ContainerID
	$ docker rm ContainerID 
	$ docker stats ContainerID 
	$ docker pause ContainerID  
	$ docker unpause ContainerID 
	$ docker kill ContainerID

# Daemon Logging	
	$ docker logs ContainerID 
	$  
  

# access container in interactive mode
  
  $ docker exec -it container_id bash


# DOCKER SECRET

Read more about docker secret commands
Use these links to read about specific commands, or continue to the example about using secrets with a service.

	$ docker secret create
	$ docker secret inspect
	$ docker secret ls
	$ docker secret rm
	--secret flag for docker service create
	--secret-add and --secret-rm flags for docker service update


# DOCKERFILE

	$ docker build 
	$ docker build  -t ImageName:TagName dir
		e.g sudo docker build –t myimage:0.1. 

		-t − is to mention a tag to the image

		ImageName − This is the name you want to give to your image.

		TagName − This is the tag you want to give to your image.

		Dir − The directory where the Docker File is present.


#  Docker Compose & Docker Swarm
	Docker Compose is a YAML file which contains details about the services, networks, and volumes for setting up the Docker application. So, you can use Docker Compose to create separate containers, host them and get them to communicate with each other. Each container will expose a port for communicating with other containers.
	
	Docker Compose is basically used to run multiple Docker Containers as a single server. Let me give you an example:

	Suppose if I have an application which requires WordPress, Maria DB and PHP MyAdmin. I can create one file which would start both the containers as a service without the need to start each one separately. It is really useful especially if you have a microservice architecture.
	It is really useful especially if you have a microservice architecture.
	
	Docker Swarm is a technique to create and maintain a cluster of Docker Engines. The Docker engines can be hosted on different nodes, and these nodes, which are in remote locations, form a Cluster when connected in Swarm mode.
	
# DOCKER COMPOSE
	
	$ docker-compose build 
	$ docker-compose up 
	$ docker-compose down 
	$ docker-compose scale=5

	
# docker-machine
	$ docker-machine create
	$ docker-machine create --driver virtualbox --help
	$ docker-machine create --driver virtualbox <machine-name> (swarm-vm1)

	$ docker-machine ip <machine-name>
	$ docker-machine start <machine-name>
	$ docker-machine stop <machine-name>

	$ docker-machine ssh <machine-name>
		e.g docker-machine stop manager1


# SWARM 

	docker swarm init --advertise-addr MANAGER_IP

	# Joining as Manager/Worker Node

	$ docker swarm join-token worker
    $ docker swarm join-token manager
   

# Manage nodes in a swarm

	$ docker node ls
	$ docker node inspect self --pretty  (only for manager nodes)

	$ docker node update --availability drain node-1  (only for manager update)
	$ docker node update --label-add foo --label-add bar=baz node-1

# Promote or demote a node (only manager nodes execute )
	
	$ docker node promote node-3 node-2
	$ docker node demote node-3 node-2

docker node promote and docker node demote are convenience commands for 
$ docker node update --role manager  
$ docker node update --role worker 
respectively.

# DOCKER SERVICE

	$ docker service create --replicas 5 -p 80:80 --name web nginx
	$ docker service create --mode global --name backend backend:latest
	$ docker service ls --filter name=frontend

	$ docker service ls
	Output:
	ID            NAME  REPLICAS  IMAGE  COMMAND
	ctolq1t4h2o8  web   0/5       nginx

	$ docker service scale frontend=50

# Scale multiple services
	$ docker service scale backend=3 frontend=5



 The docker stack command accepts only pre-built images.

 Docker Compose v2	 Docker compose v3
 Start services
 docker-compose up -d	
 docker stack deploy --compose-file=docker-compose.yml  
 Scale service
 	docker-compose scale =	
 	docker service scale =
 Shutdown	
 docker-compose down	
 docker stack rm

 Multi-host	No	Yes


# Remove old and unused Docker images

	 
		$ docker system prune :- delete ALL dangling data (i.e. In order: containers stopped, volumes without containers and images with no containers).Even unused data, with -a option.
		$ docker system prune -f
		$ docker system prune -af


		$ docker container prune
		$ docker image prune
		$ docker network prune
		$ docker volume prune


		$ docker rmi $(docker images --filter "dangling=true" -q --no-trunc)


# multi-stage builds

	With multi-stage builds, you use multiple FROM statements in your Dockerfile. Each FROM instruction can use a different base, and each of them begins a new stage of the build. 

# IMP LINKS
https://blog.couchbase.com/deploy-docker-compose-services-swarm/
https://www.edureka.co/blog/docker-networking/
https://www.edureka.co/blog/docker-swarm-cluster-of-docker-engines-for-high-availability
https://www.edureka.co/blog/docker-compose-containerizing-mean-stack-application/

https://www.callicoder.com/docker-machine-swarm-stack-golang-example/